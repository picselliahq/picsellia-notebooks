{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PicselliaTeam/picsellia-notebooks/blob/master/training-notebooks/tensorflow2/detection-segmentation/PicselliaTrainingQuickStart.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting picsellia_tf2\n",
      "  Using cached picsellia_tf2-0.10.27-py3-none-any.whl (1.6 MB)\n",
      "Collecting tensorflow==2.6.0\n",
      "  Using cached tensorflow-2.6.0-cp38-cp38-manylinux2010_x86_64.whl (458.4 MB)\n",
      "Collecting tf-models-official==2.3.0\n",
      "  Using cached tf_models_official-2.3.0-py2.py3-none-any.whl (840 kB)\n",
      "Collecting pillow==7.2.0\n",
      "  Using cached Pillow-7.2.0-cp38-cp38-manylinux1_x86_64.whl (2.2 MB)\n",
      "Collecting avro-python3==1.9.2.1\n",
      "  Using cached avro_python3-1.9.2.1-py3-none-any.whl\n",
      "Collecting contextlib2==0.6.0.post1\n",
      "  Using cached contextlib2-0.6.0.post1-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting pycocotools==2.0.2\n",
      "  Using cached pycocotools-2.0.2-cp38-cp38-linux_x86_64.whl\n",
      "Collecting lvis==0.5.3\n",
      "  Using cached lvis-0.5.3-py3-none-any.whl (14 kB)\n",
      "Collecting tf-slim==1.1.0\n",
      "  Using cached tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "Collecting six==1.15.0\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting matplotlib==3.3.4\n",
      "  Using cached matplotlib-3.3.4-cp38-cp38-manylinux1_x86_64.whl (11.6 MB)\n",
      "Requirement already satisfied: ipython>=7.16.1 in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from picsellia_tf2) (8.0.1)\n",
      "Collecting lxml==4.6.2\n",
      "  Using cached lxml-4.6.2-cp38-cp38-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting Cython==0.29.21\n",
      "  Using cached Cython-0.29.21-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
      "Collecting absl-py==0.10.0\n",
      "  Using cached absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
      "Collecting scipy==1.4.1\n",
      "  Using cached scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl (26.0 MB)\n",
      "Collecting apache-beam==2.27.0\n",
      "  Using cached apache_beam-2.27.0-cp38-cp38-manylinux2010_x86_64.whl (10.6 MB)\n",
      "Collecting keras==2.6.0\n",
      "  Using cached keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Collecting pandas<=1.1.5\n",
      "  Using cached pandas-1.1.5-cp38-cp38-manylinux1_x86_64.whl (9.3 MB)\n",
      "Collecting crcmod<2.0,>=1.7\n",
      "  Using cached crcmod-1.7-cp38-cp38-linux_x86_64.whl\n",
      "Collecting protobuf<4,>=3.12.2\n",
      "  Using cached protobuf-3.19.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting pytz>=2018.3\n",
      "  Using cached pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Collecting numpy<2,>=1.14.3\n",
      "  Using cached numpy-1.22.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Collecting requests<3.0.0,>=2.24.0\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting httplib2<0.18.0,>=0.8\n",
      "  Using cached httplib2-0.17.4-py3-none-any.whl (95 kB)\n",
      "Collecting pyarrow<3.0.0,>=0.15.1\n",
      "  Using cached pyarrow-2.0.0-cp38-cp38-manylinux2014_x86_64.whl (17.8 MB)\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Using cached hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
      "Collecting oauth2client<5,>=2.0.1\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Collecting fastavro<2,>=0.21.4\n",
      "  Using cached fastavro-1.4.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
      "Collecting future<1.0.0,>=0.18.2\n",
      "  Using cached future-0.18.2-py3-none-any.whl\n",
      "Collecting mock<3.0.0,>=1.0.1\n",
      "  Using cached mock-2.0.0-py2.py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from apache-beam==2.27.0->picsellia_tf2) (2.8.2)\n",
      "Collecting grpcio<2,>=1.29.0\n",
      "  Using cached grpcio-1.44.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "Collecting typing-extensions<3.8.0,>=3.7.0\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting pydot<2,>=1.2.0\n",
      "  Using cached pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Using cached pymongo-3.12.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (527 kB)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Using cached dill-0.3.1.1-py3-none-any.whl\n",
      "Collecting cycler>=0.10.0\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting opencv-python>=4.1.0.25\n",
      "  Using cached opencv_python-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.4 MB)\n",
      "Collecting kiwisolver>=1.1.0\n",
      "  Using cached kiwisolver-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "Collecting pyparsing>=2.4.0\n",
      "  Using cached pyparsing-3.0.7-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: setuptools>=18.0 in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from pycocotools==2.0.2->picsellia_tf2) (60.6.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from tensorflow==2.6.0->picsellia_tf2) (0.37.1)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Using cached h5py-3.1.0-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard~=2.6\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting clang~=5.0\n",
      "  Using cached clang-5.0-py3-none-any.whl\n",
      "Collecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Using cached tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl\n",
      "Collecting numpy<2,>=1.14.3\n",
      "  Using cached numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "Collecting tensorflow-addons\n",
      "  Using cached tensorflow_addons-0.16.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Collecting pyyaml\n",
      "  Using cached PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "Collecting gin-config\n",
      "  Using cached gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "Collecting kaggle>=1.3.9\n",
      "  Using cached kaggle-1.5.12-py3-none-any.whl\n",
      "Collecting tensorflow-model-optimization>=0.2.1\n",
      "  Using cached tensorflow_model_optimization-0.7.1-py2.py3-none-any.whl (234 kB)\n",
      "Collecting tensorflow-datasets\n",
      "  Using cached tensorflow_datasets-4.5.2-py3-none-any.whl (4.2 MB)\n",
      "Collecting dataclasses\n",
      "  Using cached dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Collecting psutil>=5.4.3\n",
      "  Using cached psutil-5.9.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Collecting google-api-python-client>=1.6.7\n",
      "  Downloading google_api_python_client-2.38.0-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-hub>=0.6.0\n",
      "  Using cached tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "Collecting google-cloud-bigquery>=0.31.0\n",
      "  Using cached google_cloud_bigquery-2.34.0-py2.py3-none-any.whl (206 kB)\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Using cached py_cpuinfo-8.0.0-py3-none-any.whl\n",
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.7 MB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Requirement already satisfied: pygments in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from ipython>=7.16.1->picsellia_tf2) (2.11.2)\n",
      "Requirement already satisfied: pickleshare in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from ipython>=7.16.1->picsellia_tf2) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from ipython>=7.16.1->picsellia_tf2) (4.8.0)\n",
      "Requirement already satisfied: stack-data in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from ipython>=7.16.1->picsellia_tf2) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from ipython>=7.16.1->picsellia_tf2) (5.1.1)\n",
      "Requirement already satisfied: backcall in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from ipython>=7.16.1->picsellia_tf2) (0.2.0)\n",
      "Requirement already satisfied: black in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from ipython>=7.16.1->picsellia_tf2) (22.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from ipython>=7.16.1->picsellia_tf2) (3.0.28)\n",
      "Requirement already satisfied: matplotlib-inline in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from ipython>=7.16.1->picsellia_tf2) (0.1.3)\n",
      "Requirement already satisfied: decorator in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from ipython>=7.16.1->picsellia_tf2) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from ipython>=7.16.1->picsellia_tf2) (0.18.1)\n",
      "Collecting google-auth-httplib2>=0.1.0\n",
      "  Using cached google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-api-core<3.0.0dev,>=1.21.0\n",
      "  Using cached google_api_core-2.5.0-py2.py3-none-any.whl (111 kB)\n",
      "Collecting uritemplate<5,>=3.0.1\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting google-auth<3.0.0dev,>=1.16.0\n",
      "  Using cached google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n",
      "Collecting packaging>=14.3\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.4.1\n",
      "  Using cached google_cloud_core-2.2.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting proto-plus>=1.10.0\n",
      "  Using cached proto_plus-1.20.3-py3-none-any.whl (46 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-2.3.0-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.8/76.8 KB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docopt\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.16.1->picsellia_tf2) (0.8.3)\n",
      "Collecting certifi\n",
      "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Collecting urllib3\n",
      "  Using cached urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-6.1.0-py2.py3-none-any.whl (9.2 kB)\n",
      "Collecting pbr>=0.11\n",
      "  Using cached pbr-5.8.1-py2.py3-none-any.whl (113 kB)\n",
      "Collecting pyasn1>=0.1.7\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting pyasn1-modules>=0.0.5\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa>=3.1.4\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from pexpect>4.3->ipython>=7.16.1->picsellia_tf2) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.16.1->picsellia_tf2) (0.2.5)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting dm-tree~=0.1.1\n",
      "  Using cached dm_tree-0.1.6-cp38-cp38-manylinux_2_24_x86_64.whl (94 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from black->ipython>=7.16.1->picsellia_tf2) (2.5.1)\n",
      "INFO: pip is looking at multiple versions of backcall to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting backcall\n",
      "  Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "INFO: pip is looking at multiple versions of wrapt to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of wheel to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting wheel~=0.35\n",
      "  Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "INFO: pip is looking at multiple versions of typing-extensions to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting typing-extensions<3.8.0,>=3.7.0\n",
      "  Downloading typing_extensions-3.7.4.2-py3-none-any.whl (22 kB)\n",
      "  Downloading typing_extensions-3.7.4.1-py3-none-any.whl (20 kB)\n",
      "  Downloading typing_extensions-3.7.4-py3-none-any.whl (20 kB)\n",
      "INFO: pip is looking at multiple versions of traitlets to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting traitlets>=5\n",
      "  Using cached traitlets-5.1.1-py3-none-any.whl (102 kB)\n",
      "INFO: pip is looking at multiple versions of typing-extensions to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of termcolor to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of tensorflow-model-optimization to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow-model-optimization>=0.2.1\n",
      "  Downloading tensorflow_model_optimization-0.7.0-py2.py3-none-any.whl (213 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.1/213.1 KB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tensorflow-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow-hub>=0.6.0\n",
      "  Downloading tensorflow_hub-0.11.0-py2.py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m107.2/107.2 KB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of tensorflow-estimator to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m463.1/463.1 KB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tensorboard to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests<3.0.0,>=2.24.0\n",
      "  Downloading requests-2.27.0-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.1/63.1 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pytz to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pytz>=2018.3\n",
      "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.8/510.8 KB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of python-dateutil to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting python-dateutil<3,>=2.8.0\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "INFO: pip is looking at multiple versions of pyparsing to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyparsing>=2.4.0\n",
      "  Downloading pyparsing-3.0.6-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m97.6/97.6 KB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pymongo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Downloading pymongo-3.12.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (527 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m528.0/528.0 KB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pydot to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pydot<2,>=1.2.0\n",
      "  Downloading pydot-1.4.1-py2.py3-none-any.whl (19 kB)\n",
      "INFO: pip is looking at multiple versions of pyarrow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyarrow<3.0.0,>=0.15.1\n",
      "  Downloading pyarrow-1.0.1-cp38-cp38-manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of py-cpuinfo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-7.0.0.tar.gz (95 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.9/95.9 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of psutil to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting psutil>=5.4.3\n",
      "  Downloading psutil-5.8.0-cp38-cp38-manylinux2010_x86_64.whl (296 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m296.0/296.0 KB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of protobuf to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting protobuf<4,>=3.12.2\n",
      "  Downloading protobuf-3.19.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of prompt-toolkit to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Using cached prompt_toolkit-3.0.28-py3-none-any.whl (380 kB)\n",
      "INFO: pip is looking at multiple versions of pexpect to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pexpect>4.3\n",
      "  Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "INFO: pip is looking at multiple versions of opt-einsum to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv-python>=4.1.0.25\n",
      "  Downloading opencv_python-4.5.4.60-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.3/60.3 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of oauth2client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting oauth2client<5,>=2.0.1\n",
      "  Downloading oauth2client-4.1.2-py2.py3-none-any.whl (99 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.3/99.3 KB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy<2,>=1.14.3\n",
      "  Downloading numpy-1.19.4-cp38-cp38-manylinux2010_x86_64.whl (14.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of mock to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mock<3.0.0,>=1.0.1\n",
      "  Downloading mock-1.3.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of kiwisolver to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kiwisolver>=1.1.0\n",
      "  Downloading kiwisolver-1.3.1-cp38-cp38-manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of keras-preprocessing to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of kaggle to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kaggle>=1.3.9\n",
      "  Downloading kaggle-1.5.10.tar.gz (59 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of jedi to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jedi>=0.16\n",
      "  Using cached jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
      "INFO: pip is looking at multiple versions of httplib2 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting httplib2<0.18.0,>=0.8\n",
      "  Downloading httplib2-0.17.3-py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of hdfs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.5.8.tar.gz (41 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.9/41.9 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of h5py to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of grpcio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio<2,>=1.29.0\n",
      "  Downloading grpcio-1.43.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of google-pasta to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of google-cloud-bigquery to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-bigquery>=0.31.0\n",
      "  Downloading google_cloud_bigquery-2.33.0-py2.py3-none-any.whl (205 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m205.8/205.8 KB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of google-api-python-client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-api-python-client>=1.6.7\n",
      "  Using cached google_api_python_client-2.37.0-py2.py3-none-any.whl (8.1 MB)\n",
      "INFO: pip is looking at multiple versions of future to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of flatbuffers to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of fastavro to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fastavro<2,>=0.21.4\n",
      "  Downloading fastavro-1.4.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of dill to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of cycler to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting cycler>=0.10.0\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "INFO: pip is looking at multiple versions of crcmod to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of clang to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of astunparse to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pandas<=1.1.5\n",
      "  Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB)\n",
      "INFO: pip is looking at multiple versions of ipython to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting ipython>=7.16.1\n",
      "  Using cached ipython-8.0.1-py3-none-any.whl (747 kB)\n",
      "  Using cached ipython-8.0.0-py3-none-any.whl (747 kB)\n",
      "  Using cached ipython-7.31.1-py3-none-any.whl (792 kB)\n",
      "Collecting typeguard>=2.7\n",
      "  Using cached typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Collecting promise\n",
      "  Using cached promise-2.3-py3-none-any.whl\n",
      "Collecting tensorflow-metadata\n",
      "  Using cached tensorflow_metadata-1.6.0-py3-none-any.whl (48 kB)\n",
      "Collecting importlib-resources\n",
      "  Using cached importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.55.0-py2.py3-none-any.whl (212 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.3/212.3 KB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Using cached grpcio_status-1.44.0-py3-none-any.whl (10.0 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Using cached google_crc32c-1.3.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (37 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Using cached importlib_metadata-4.11.1-py3-none-any.whl (17 kB)\n",
      "Collecting zipp>=3.1.0\n",
      "  Using cached zipp-3.7.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: wrapt, typing-extensions, text-unidecode, termcolor, tensorflow-estimator, tensorboard-plugin-wit, sentencepiece, pytz, pyasn1, py-cpuinfo, keras, httplib2, gin-config, flatbuffers, docopt, dataclasses, crcmod, clang, certifi, zipp, werkzeug, urllib3, uritemplate, typeguard, tqdm, tensorboard-data-server, six, rsa, pyyaml, python-slugify, pyparsing, pymongo, pyasn1-modules, psutil, protobuf, pillow, pbr, oauthlib, numpy, lxml, kiwisolver, idna, google-crc32c, gast, future, fastavro, dill, Cython, cycler, contextlib2, charset-normalizer, cachetools, avro-python3, tensorflow-hub, tensorflow-addons, scipy, requests, pydot, pyarrow, proto-plus, promise, packaging, opt-einsum, opencv-python-headless, opencv-python, oauth2client, mock, keras-preprocessing, ipython, importlib-resources, importlib-metadata, h5py, grpcio, googleapis-common-protos, google-resumable-media, google-pasta, google-auth, dm-tree, astunparse, absl-py, tf-slim, tensorflow-model-optimization, tensorflow-metadata, requests-oauthlib, pandas, matplotlib, markdown, kaggle, hdfs, grpcio-status, google-auth-httplib2, google-api-core, tensorflow-datasets, pycocotools, lvis, google-cloud-core, google-auth-oauthlib, google-api-python-client, apache-beam, tensorboard, google-cloud-bigquery, tensorflow, tf-models-official, picsellia_tf2\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.1.1\n",
      "    Uninstalling typing_extensions-4.1.1:\n",
      "      Successfully uninstalled typing_extensions-4.1.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 8.0.1\n",
      "    Uninstalling ipython-8.0.1:\n",
      "      Successfully uninstalled ipython-8.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "black 22.1.0 requires typing-extensions>=3.10.0.0; python_version < \"3.10\", but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Cython-0.29.21 absl-py-0.10.0 apache-beam-2.27.0 astunparse-1.6.3 avro-python3-1.9.2.1 cachetools-5.0.0 certifi-2021.10.8 charset-normalizer-2.0.12 clang-5.0 contextlib2-0.6.0.post1 crcmod-1.7 cycler-0.11.0 dataclasses-0.6 dill-0.3.1.1 dm-tree-0.1.6 docopt-0.6.2 fastavro-1.4.9 flatbuffers-1.12 future-0.18.2 gast-0.4.0 gin-config-0.5.0 google-api-core-2.5.0 google-api-python-client-2.38.0 google-auth-2.6.0 google-auth-httplib2-0.1.0 google-auth-oauthlib-0.4.6 google-cloud-bigquery-2.34.0 google-cloud-core-2.2.2 google-crc32c-1.3.0 google-pasta-0.2.0 google-resumable-media-2.3.0 googleapis-common-protos-1.55.0 grpcio-1.44.0 grpcio-status-1.44.0 h5py-3.1.0 hdfs-2.6.0 httplib2-0.17.4 idna-3.3 importlib-metadata-4.11.1 importlib-resources-5.4.0 ipython-7.31.1 kaggle-1.5.12 keras-2.6.0 keras-preprocessing-1.1.2 kiwisolver-1.3.2 lvis-0.5.3 lxml-4.6.2 markdown-3.3.6 matplotlib-3.3.4 mock-2.0.0 numpy-1.19.5 oauth2client-4.1.3 oauthlib-3.2.0 opencv-python-4.5.5.62 opencv-python-headless-4.5.5.62 opt-einsum-3.3.0 packaging-21.3 pandas-1.1.5 pbr-5.8.1 picsellia_tf2-0.10.27 pillow-7.2.0 promise-2.3 proto-plus-1.20.3 protobuf-3.19.4 psutil-5.9.0 py-cpuinfo-8.0.0 pyarrow-2.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycocotools-2.0.2 pydot-1.4.2 pymongo-3.12.3 pyparsing-3.0.7 python-slugify-6.1.0 pytz-2021.3 pyyaml-6.0 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 scipy-1.4.1 sentencepiece-0.1.96 six-1.15.0 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.6.0 tensorflow-addons-0.16.1 tensorflow-datasets-4.5.2 tensorflow-estimator-2.8.0 tensorflow-hub-0.12.0 tensorflow-metadata-1.6.0 tensorflow-model-optimization-0.7.1 termcolor-1.1.0 text-unidecode-1.3 tf-models-official-2.3.0 tf-slim-1.1.0 tqdm-4.62.3 typeguard-2.13.3 typing-extensions-3.7.4.3 uritemplate-4.1.1 urllib3-1.26.8 werkzeug-2.0.3 wrapt-1.12.1 zipp-3.7.0\n",
      "Collecting picsellia\n",
      "  Using cached picsellia-5.1.1-py3-none-any.whl (65 kB)\n",
      "Collecting requests==2.24.0\n",
      "  Using cached requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting tqdm==4.62.2\n",
      "  Using cached tqdm-4.62.2-py2.py3-none-any.whl (76 kB)\n",
      "Collecting beartype==0.9.1\n",
      "  Using cached beartype-0.9.1-py3-none-any.whl (524 kB)\n",
      "Collecting rich==11.2.0\n",
      "  Using cached rich-11.2.0-py3-none-any.whl (217 kB)\n",
      "Requirement already satisfied: Pillow==7.2.0 in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from picsellia) (7.2.0)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from requests==2.24.0->picsellia) (2021.10.8)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (from rich==11.2.0->picsellia) (2.11.2)\n",
      "Collecting colorama<0.5.0,>=0.4.0\n",
      "  Using cached colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: commonmark, chardet, urllib3, tqdm, idna, colorama, beartype, rich, requests, picsellia\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.8\n",
      "    Uninstalling urllib3-1.26.8:\n",
      "      Successfully uninstalled urllib3-1.26.8\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.62.3\n",
      "    Uninstalling tqdm-4.62.3:\n",
      "      Successfully uninstalled tqdm-4.62.3\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.3\n",
      "    Uninstalling idna-3.3:\n",
      "      Successfully uninstalled idna-3.3\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.27.1\n",
      "    Uninstalling requests-2.27.1:\n",
      "      Successfully uninstalled requests-2.27.1\n",
      "Successfully installed beartype-0.9.1 chardet-3.0.4 colorama-0.4.4 commonmark-0.9.1 idna-2.10 picsellia-5.1.1 requests-2.24.0 rich-11.2.0 tqdm-4.62.2 urllib3-1.25.11\n"
     ]
    }
   ],
   "source": [
    "!pip install picsellia_tf2\n",
    "!pip install picsellia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¥‘ **Welcome To Picsellia Training Quickstart Notebook** ğŸ¥‘\n",
    "\n",
    "In this Notebook, you will see how to launch a training from a created experiment on the Platform and log all the evaluation metrics to analyse your trained model.\n",
    "\n",
    "**Step 1, let's import our python SDK and our tensorflow2 wrapper** \n",
    "\n",
    "If you do not have our packages you can run: \n",
    "- pip install picsellia picsellia_tf2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ubuntu/.virtualenvs/build/lib/python3.8/site-packages (1.19.5)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.22.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.22.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.22.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while loading conf file for logging. No logging done.\n"
     ]
    }
   ],
   "source": [
    "from picsellia import Client\n",
    "from picsellia_tf2 import pxl_utils\n",
    "from picsellia_tf2 import pxl_tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2, fetch your experiment files from the Picsellia servers**\n",
    "\n",
    "Every experiment you make has an unique identifier allowing you to retrieve all the necessary informations with one command.\n",
    "\n",
    "By passing `tree` and `with_files` to True, you will automatically get a folder architecture like:\n",
    "\n",
    "- `experiment_name`/\n",
    "    - checkpoint/\n",
    "    - config/\n",
    "    - exported_model/\n",
    "    - images/\n",
    "    - metrics/\n",
    "    - records/\n",
    "    - results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while loading conf file for logging. No logging done.\n",
      "capture/checkpoint\n",
      "config\n",
      "model-latest\n",
      "|\u001b[92mâ–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬\u001b[0m|â—‰ 100% 10582337/10582337    \n",
      "--*--\n",
      "checkpoint-data-latest\n",
      "|\u001b[92mâ–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬\u001b[0m|â—‰ 100% 10752177/10752177    \n",
      "--*--\n",
      "checkpoint-index-latest\n",
      "logs\n"
     ]
    }
   ],
   "source": [
    "from picsellia import Client\n",
    "from picsellia_tf2 import pxl_utils\n",
    "from picsellia_tf2 import pxl_tf\n",
    "import os\n",
    "\n",
    "api_token = ''\n",
    "project_name = ''\n",
    "experiment_name = ''\n",
    "\n",
    "client = Client(\n",
    "    api_token=api_token,\n",
    "    organization=None # Set to an organization name if you want to checkout an other org.\n",
    ")\n",
    "\n",
    "project= client.get_project(project_name)\n",
    "\n",
    "experiment = project.get_experiment(\n",
    "    name=experiment_name, \n",
    "    tree=True, \n",
    "    with_artifacts=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3, Dataset fetching** \n",
    "\n",
    "Download all the necessary data for your training:\n",
    "- Dataset\n",
    "- Annotations\n",
    "\n",
    "Then generate the labelmap for your model, `labelmap` is needed for Picsellia to map your verbose labels *(i.e \"cat\", \"dog\", \"hot-dog\")* to your categorical labels *(i.e 1, 2, 3)*.\n",
    "\n",
    ">\n",
    "> You can find more info about the labelmap format [here](https://google.com)\n",
    ">\n",
    "\n",
    "Finally performing train-test-split to perform training, `train_test_split`  is recommended to be stored inside Picsellia, this way you will be able to access the validation interface and have full visibility over your training data ğŸ‘Š\n",
    "\n",
    "If you want to know more about our train_test_split format, here is the [documentation page](https://google.com)\n",
    "**(default repartition is 0.8 / 0.2 from train/test)**\n",
    "\n",
    "Once the train test set created, we send the repartition to Picsellia platform in order to visualize it later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "|\u001b[92mâ–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬\u001b[0m|â—‰ 100% 129/129    \n",
      "--*--\n"
     ]
    }
   ],
   "source": [
    "experiment.download_annotations()\n",
    "experiment.download_pictures()\n",
    "experiment.generate_labelmap()\n",
    "experiment.log('labelmap', experiment.label_map, 'labelmap', replace=True)\n",
    "train_list, eval_list, train_list_id, eval_list_id, label_train, label_eval, categories = experiment.train_test_split()\n",
    "\n",
    "train_split = {\n",
    "    'x': categories,\n",
    "    'y': label_train,\n",
    "    'image_list': train_list_id\n",
    "}\n",
    "experiment.log('train-split', train_split, 'bar', replace=True)\n",
    "\n",
    "test_split = {\n",
    "    'x': categories,\n",
    "    'y': label_eval,\n",
    "    'image_list': eval_list_id\n",
    "}\n",
    "experiment.log('test-split', test_split, 'bar', replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4, Pre-processing** \n",
    "\n",
    "Now we will create the necessary record files to perform training, and initialize the training with the parameters that you chose for your experiment on Picsellia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating record file at capture/records/train.record\n",
      "annotation type used for the variable generator: rectangle\n",
      "Successfully created the TFRecords: capture/records/train.record\n",
      "Creating record file at capture/records/eval.record\n",
      "annotation type used for the variable generator: rectangle\n",
      "Successfully created the TFRecords: capture/records/eval.record\n",
      "learning_rate {\n",
      "  exponential_decay_learning_rate {\n",
      "    initial_learning_rate: 0.0010000000474974513\n",
      "    decay_steps: 500\n",
      "    decay_factor: 0.8999999761581421\n",
      "    staircase: true\n",
      "  }\n",
      "}\n",
      "momentum_optimizer_value: 0.8999999761581421\n",
      "\n",
      "Configuration successfully edited and saved at capture/config\n"
     ]
    }
   ],
   "source": [
    "parameters = experiment.get_log(name='parameters')\n",
    "\n",
    "pxl_utils.create_record_files(\n",
    "        dict_annotations=experiment.dict_annotations, \n",
    "        train_list=train_list, \n",
    "        train_list_id=train_list_id, \n",
    "        eval_list=eval_list, \n",
    "        eval_list_id=eval_list_id,\n",
    "        label_path=experiment.label_path, \n",
    "        record_dir=experiment.record_dir, \n",
    "        tfExample_generator=pxl_tf.tf_vars_generator, \n",
    "        annotation_type=parameters['annotation_type']\n",
    "        )\n",
    "    \n",
    "pxl_utils.edit_config(\n",
    "        model_selected=experiment.checkpoint_dir, \n",
    "        input_config_dir=experiment.config_dir,\n",
    "        output_config_dir=experiment.config_dir,\n",
    "        record_dir=experiment.record_dir, \n",
    "        label_map_path=experiment.label_path, \n",
    "        num_steps=parameters[\"steps\"],\n",
    "        batch_size=parameters['batch_size'],\n",
    "        learning_rate=parameters['learning_rate'],\n",
    "        annotation_type=parameters['annotation_type'],\n",
    "        eval_number=5,\n",
    "        parameters=parameters,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5, Training** \n",
    "\n",
    "Then just launch training, and go grab a cup of coffee :D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pxl_utils.train(\n",
    "        ckpt_dir=experiment.checkpoint_dir, \n",
    "        config_dir=experiment.config_dir,\n",
    "        log_real_time=experiment,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6, Evaluation**\n",
    "\n",
    "Now let's run evaluation on your trained model in order to analyse the performances later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pxl_utils.export_graph(\n",
    "    ckpt_dir=experiment.checkpoint_dir, \n",
    "    exported_model_dir=experiment.exported_model_dir, \n",
    "    config_dir=experiment.config_dir\n",
    ")\n",
    "pxl_utils.evaluate(\n",
    "    experiment.metrics_dir, \n",
    "    experiment.config_dir, \n",
    "    experiment.checkpoint_dir\n",
    ")  \n",
    "conf, evaluation = pxl_utils.get_confusion_matrix(\n",
    "    input_tfrecord_path=os.path.join(experiment.record_dir, 'eval.record'),\n",
    "    model=os.path.join(experiment.exported_model_dir, 'saved_model'),\n",
    "    labelmap=experiment.label_map\n",
    ")\n",
    "\n",
    "\n",
    "confusion = {\n",
    "    'categories': list(experiment.label_map.values()),\n",
    "    'values': conf.tolist()\n",
    "}\n",
    "\n",
    "experiment.log('confusion-matrix', confusion, 'heatmap', replace=True)\n",
    "experiment.log('evaluation', evaluation, 'evaluation', replace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7, Exporting and Inference**\n",
    "\n",
    "This part will export your trained model as saved_model in order to use in production or for inference in Picsellia. \n",
    "\n",
    "Inference will be performed on several images of your test set and sent to Picsellia platform to visualize some results and share it with your colloborators or community. \n",
    "\n",
    "Then all the evaluation metrics will be uploaded to your experiments pages in order to visualize all your graphs :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pxl_utils.infer(\n",
    "    experiment.record_dir, \n",
    "    exported_model_dir=experiment.exported_model_dir, \n",
    "    label_map_path=os.path.join(experiment.base_dir,'label_map.pbtxt'), \n",
    "    results_dir=experiment.results_dir, \n",
    "    from_tfrecords=True, \n",
    "    disp=False\n",
    "    )\n",
    "\n",
    "metrics = pxl_utils.tf_events_to_dict('{}/metrics'.format(experiment.experiment_name), 'eval')\n",
    "logs = pxl_utils.tf_events_to_dict('{}/checkpoint'.format(experiment.experiment_name), 'train')\n",
    "for variable in logs.keys():\n",
    "    data = {\n",
    "        'steps': logs[variable][\"steps\"],\n",
    "        'values': logs[variable][\"values\"]\n",
    "    }\n",
    "    experiment.log('-'.join(variable.split('/')), data, 'line', replace=True)\n",
    "experiment.log('metrics', metrics, 'table', replace=True)\n",
    "experiment.store('model-latest')\n",
    "experiment.store('config')\n",
    "experiment.store('checkpoint-data-latest')\n",
    "experiment.store('checkpoint-index-latest')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.google.com/search?q=google+image&client=ubuntu&hs=Q7r&channel=fs&source=lnms&tbm=isch&sa=X&ved=2ahUKEwii17Hsr9juAhWlnVwKHRUoD0sQ_AUoAXoECBQQAw&biw=2560&bih=931#imgrc=TMmIvimt9rgaYM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
